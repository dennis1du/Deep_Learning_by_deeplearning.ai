{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "- Load GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    \n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "- Load the 50-dimensional GloVe vectors to represent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('./glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    '''\n",
    "    Compute the cosine similarity between u and v vectors\n",
    "    \n",
    "    Arguments:\n",
    "        --u: a word vector of shape (n,)\n",
    "        --v: a word vector of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "    cosine similarity of u and v\n",
    "    '''\n",
    "    # Compute numerator\n",
    "    num = np.dot(u, v)\n",
    "    \n",
    "    # Compute denomenator\n",
    "    den = np.sqrt(np.sum(u ** 2)) * np.sqrt(np.sum(v ** 2))\n",
    "    \n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogy\n",
    "* Task: <font color='brown'>\"*a* is to *b* as *c* is to **?**\"</font> \n",
    "* We are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n",
    "    $e_b - e_a \\approx e_d - e_c$\n",
    "* We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    '''\n",
    "    Find the best word such that \"a is to b as c is to ?\"\n",
    "    \n",
    "    Arguments:\n",
    "        --word_a: string\n",
    "        --word_b: srting\n",
    "        --word_c: string\n",
    "    \n",
    "    Returns:\n",
    "    best_word: the word such that e_b - e_a and e_bestword - e_c are the closest\n",
    "    '''\n",
    "    \n",
    "    # Convert words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Retrieve word embeddings\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    # Initialization\n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = float('-inf')\n",
    "    best_word = None\n",
    "    \n",
    "    # Avoid output one of the input words\n",
    "    input_words_set = set([word_a, word_b, word_c])\n",
    "    \n",
    "    # Loop over the whole word vector set\n",
    "    for word in words:\n",
    "        if word in input_words_set:\n",
    "            continue\n",
    "        \n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[word] - e_c)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = word\n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, word_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing word vectors\n",
    "* examine gender biases that can be reflected in a word embedding\n",
    "* explore algorithm for reducing the bias\n",
    "\n",
    "### Examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with the difference vector:\n",
      "\n",
      "john -0.23163356145973724\n",
      "marie 0.315597935396073\n",
      "sophie 0.31868789859418784\n",
      "ronaldo -0.31244796850329437\n",
      "priya 0.17632041839009402\n",
      "rahul -0.16915471039231716\n",
      "danielle 0.24393299216283895\n",
      "reza -0.07930429672199553\n",
      "katy 0.2831068659572615\n",
      "yasmin 0.23313857767928758\n"
     ]
    }
   ],
   "source": [
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "\n",
    "print('List of names and their similarities with the difference vector:\\n')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "for name in name_list:\n",
    "    print (name, cosine_similarity(word_to_vec_map[name], g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities with the difference vector:\n",
      "\n",
      "lipstick 0.2769191625638267\n",
      "guns -0.1888485567898898\n",
      "science -0.06082906540929701\n",
      "arts 0.008189312385880337\n",
      "literature 0.06472504433459932\n",
      "warrior -0.20920164641125288\n",
      "doctor 0.11895289410935041\n",
      "tree -0.07089399175478091\n",
      "receptionist 0.33077941750593737\n",
      "technology -0.13193732447554302\n",
      "fashion 0.03563894625772699\n",
      "teacher 0.17920923431825664\n",
      "engineer -0.0803928049452407\n",
      "pilot 0.0010764498991916937\n",
      "computer -0.10330358873850498\n",
      "singer 0.1850051813649629\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities with the difference vector:\\n')\n",
    "\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for word in word_list:\n",
    "    print (word, cosine_similarity(word_to_vec_map[word], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiasing Algorithm\n",
    "Note that those gender-specific pairs such as \"grandmother/grandfather\" should remain unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) **Neutralization**: remove the bias component by projecting the word vector on the space orthogonal to the bias axis\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g\\tag{2}$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}\\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    '''\n",
    "    Remove the bias component in the word for g vector and return the debiased vector\n",
    "    \n",
    "    Arguments:\n",
    "        --word: string, input word\n",
    "        --g: numpy array of shape (50,), corresponding to the bias axis (e.g. gender)\n",
    "    \n",
    "    Returns:\n",
    "    e_debiased: debiased vector\n",
    "    '''\n",
    "    \n",
    "    # Retrieve word embeddings\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute the bias component\n",
    "    e_bias = np.dot(e, g) / np.sum(g ** 2) * g\n",
    "    \n",
    "    # Neutralize e by substracting the bias component\n",
    "    e_debiased = e - e_bias\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.33077941750593737\n",
      "cosine similarity between receptionist and g, after neutralizing:  -2.6832242276243644e-17\n"
     ]
    }
   ],
   "source": [
    "e = \"receptionist\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) **Equalization**: make a pair of words are equidistant from the other axes, except the bias axis, so that they differ only through one property (e.g. gender).\n",
    "\n",
    "Ex. by applying neutralizing to \"babysit\" we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that \"actor\" and \"actress\" are equidistant from \"babysit.\" The equalization algorithm takes care of this.\n",
    "\n",
    "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}\\tag{4}$$ \n",
    "\n",
    "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{5}$$ \n",
    "\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B} \\tag{6}$$\n",
    "\n",
    "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{7}$$ \n",
    "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{8}$$\n",
    "\n",
    "\n",
    "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||} \\tag{9}$$\n",
    "\n",
    "\n",
    "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||} \\tag{10}$$\n",
    "\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} \\tag{11}$$\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} \\tag{12}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    '''\n",
    "    Debias gender specific pair of words by making them equidistant from the space orthogonal to the bias axis\n",
    "    \n",
    "    Arguments:\n",
    "        --pair: tuple of 2, input pair of strings\n",
    "        --bias_axis: numpy aray of shape (50,), vector corresponding to the bias axis (e.g. gender)\n",
    "    \n",
    "    Returns:\n",
    "    (e1, e2): the pair of debiased word vectors\n",
    "    '''\n",
    "    \n",
    "    # Retrieve word embeddings\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    # Compute the mean\n",
    "    mu = (e_w1 + e_w2) / 2\n",
    "    \n",
    "    # Compute the projections of mu over the bias axis and the orthogonal axis\n",
    "    mu_bias = np.dot(mu, bias_axis) / np.sum(bias_axis ** 2) * bias_axis\n",
    "    mu_orth = mu - mu_bias\n",
    "    \n",
    "    # Compute the projections of e_w1 and e_w2 over the bias axis\n",
    "    e_w1_bias = np.dot(e_w1, bias_axis) / np.sum(bias_axis ** 2) * bias_axis\n",
    "    e_w2_bias = np.dot(e_w2, bias_axis) / np.sum(bias_axis ** 2) * bias_axis\n",
    "    \n",
    "    # Correct the bias components for e_w1 and e_w2\n",
    "    corrected_e_w1_bias = np.sqrt(np.abs(1 - np.sum(mu_orth ** 2))) * (e_w1_bias - mu_bias) / np.sqrt(np.sum((e_w1 - mu_orth - mu_bias) ** 2))\n",
    "    corrected_e_w2_bias = np.sqrt(np.abs(1 - np.sum(mu_orth ** 2))) * (e_w2_bias - mu_bias) / np.sqrt(np.sum((e_w2 - mu_orth - mu_bias) ** 2))\n",
    "    \n",
    "    # Debias by equializing e1 and e2 to the sum of their corrected projections\n",
    "    e1 = corrected_e_w1_bias + mu_orth\n",
    "    e2 = corrected_e_w2_bias + mu_orth\n",
    "    \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.11711095765336832\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.35666618846270376\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.7004364289309387\n",
      "cosine_similarity(e2, gender) =  0.7004364289309387\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
