{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tqdm import tqdm\n",
    "from faker import Faker\n",
    "from babel.dates import format_date\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load helper functions\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from nmt_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    \n",
    "    Arguments:\n",
    "        --x : tensor\n",
    "        --axis: integer, axis along which the softmax normalization is applied\n",
    "    \n",
    "    Returns:\n",
    "    tensor, output of softmax transformation\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: in case `dim(x) == 1`\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date).\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "    - **Note**: These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:25<00:00, 116.53it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('08 jul 2008', '2008-07-08'),\n",
       " ('8 sep 1999', '1999-09-08'),\n",
       " ('thursday january 1 1981', '1981-01-01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "- map the raw text data into the index values. \n",
    "    - set Tx=30 \n",
    "        - We assume Tx is the maximum length of the human readable date.\n",
    "        - If input is longer than Tx: truncate it; if input is shorter than Tx: pad it\n",
    "    - set Ty=10\n",
    "        - \"YYYY-MM-DD\" is 10 characters long.\n",
    "- processed data\n",
    "    - `X`: a processed version of the human readable dates in the training set\n",
    "        - Each character in X is replaced by an index (integer) mapped to the character using `human_vocab`\n",
    "        - Each date is padded to ensure a length of $T_x$ using a special character (< pad >)\n",
    "        - shape = (m, Tx), where m is the number of training examples in a batch\n",
    "    - `y`: a processed version of the machine readable dates in the training set\n",
    "        - Each character is replaced by the index (integer) it is mapped to in `machine_vocab` \n",
    "        - shape = (m, Ty)\n",
    "    - `X_1h`: one-hot version of X\n",
    "        - Each index in X_1h is converted to the one-hot representation\n",
    "        - shape = (m, Tx, len(human_vocab))\n",
    "    - `y_1h`: one-hot version of Y\n",
    "        - Each index in y_1h is converted to the one-hot representation\n",
    "        - shape = (m, Tx, len(machine_vocab)) \n",
    "        - len(machine_vocab) = 11 since there are 10 numeric digits (0 to 9) and the `-` symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "y.shape: (10000, 10)\n",
      "X_1h.shape: (10000, 30, 37)\n",
      "y_1h.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "\n",
    "X, y, X_1h, y_1h = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "print(\"X_1h.shape:\", X_1h.shape)\n",
    "print(\"y_1h.shape:\", y_1h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Attention\n",
    "## Attention mechanism\n",
    "- One-step attention function\n",
    "    - compute the attention weights/variables $\\alpha^{\\langle t, t' \\rangle}$\n",
    "    - use $\\alpha^{\\langle t, t' \\rangle}$ to compute context variable $context^{\\langle t \\rangle}$ for each time-step as the input of the post-attention LSTM networks\n",
    "\n",
    "## Pre-attention and Post-attention LSTMs\n",
    "- There are two separate LSTMs in the model: pre-attention and post-attention LSTMs\n",
    "- Pre-attention Bi-LSTM:\n",
    "    - the pre-attention Bi-LSTMs comes before the attention mechanism \n",
    "    - the pre-attention Bi-LSTMs goes through $T_x$ time-steps\n",
    "    - Inputs: $x^{\\langle t' \\rangle}$\n",
    "    - Outputs: $a^{\\langle t' \\rangle} = [\\overrightarrow{a}^{\\langle t' \\rangle}, \\overleftarrow{a}^{\\langle t' \\rangle}]$, concatenation of the hidden states of both forward-direction and backward-direction\n",
    "- Post-attention LSTM:\n",
    "    - the post-attention LSTMs comes after the attention mechanism\n",
    "    - the post-attention LSTMs goes through $T_y$ time-steps\n",
    "    - the post-attention LSTMs passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$\n",
    "    - Inputs: $context^{\\langle t \\rangle}$, which is determined by $a^{\\langle t' \\rangle}$ and $s^{\\langle t-1 \\rangle}$\n",
    "    - Outputs: $y^{\\langle t \\rangle}$\n",
    "- Note\n",
    "    - in this model, the post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input\n",
    "    - the post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input \n",
    "    - we have designed the model this way because unlike language generation (where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date\n",
    "\n",
    "## Compute \"energies\" $e^{\\langle t, t' \\rangle}$\n",
    "- \"energies\" variable $e^{\\langle t, t' \\rangle}$ is computed as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$\n",
    "    - $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n",
    "    - $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM\n",
    "- $s^{\\langle t \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$\n",
    "- $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $\\alpha^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$ (i.e. $x^{\\langle t' \\rangle}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**one_step_attention()**\n",
    "- since the `model()` will call the layers in `one_step_attention()` Ty times, it's important that all Ty copies have the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shared layers as global variables\n",
    "## RepeatVector()\n",
    "repeator = tf.keras.layers.RepeatVector(n=Tx)\n",
    "\n",
    "## Concatenate()\n",
    "concatenator = tf.keras.layers.Concatenate(axis=-1)\n",
    "\n",
    "## Dense()\n",
    "densor1 = tf.keras.layers.Dense(units=10, activation='tanh')  ### compute \"intermediate energies\" variables\n",
    "densor2 = tf.keras.layers.Dense(units=1, activation='relu')   ### compute \"energies\" variables \n",
    "\n",
    "## Activation()\n",
    "activator = tf.keras.layers.Activation(softmax, name='attention_weights')\n",
    "\n",
    "## Dot()\n",
    "dotor = tf.keras.layers.Dot(axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    '''\n",
    "    Implement one step for attention: compute the context vector (computed as the dot product of \n",
    "    the attention weights variable \"alpha\" and the hidden states \"a\" of the pre-attention Bi-LSTM)\n",
    "    \n",
    "    Arguments:\n",
    "        --a: numpy array of shape (m, Tx, 2*n_a), hidden states output of the pre-attention Bi-LSTM\n",
    "        --s_prev: numpy array of shape (m, n_s), previous hidden state of the post-attention LSTM\n",
    "    \n",
    "    Returns:\n",
    "    context: context vector, input for the post-attention LSTM cell\n",
    "    '''\n",
    "    \n",
    "    # Use repeator to repeat s_prev to shape (m, Tx, n_s) so that we can concatenate it with all hidden states \"a\"\n",
    "    s_prev = repeator(s_prev)\n",
    "    \n",
    "    # Use concatenator to concatenate a and s_prev on the last axis\n",
    "    concat = concatenator([a, s_prev])\n",
    "    \n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute \"intermediate energies\" variable e\n",
    "    e = densor1(concat)\n",
    "    \n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute \"energies\" variable\n",
    "    energies = densor2(e)\n",
    "    \n",
    "    # Use activator on energies to compute the attention weights \"alpha\"\n",
    "    alpha = activator(energies)\n",
    "    \n",
    "    # Use dotor together with a and alpha to compute the context vector\n",
    "    context = dotor([alpha, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model()**\n",
    "- First,`model()` runs the input X through the pre-attention Bi-LSTM to get $[a^{\\langle 1 \\rangle},a^{\\langle 2 \\rangle}, ..., a^{\\langle T_x \\rangle}]$\n",
    "- Then, `model()` calls `one_step_attention()` $T_y$ times using a `for` loop.  At each iteration of this loop:\n",
    "    - gives the computed context vector $context^{\\langle t \\rangle}$ to the post-attention LSTM.\n",
    "    - runs the output of the post-attention LSTM through a dense layer with softmax activation.\n",
    "    - softmax generates a prediction $\\hat{y}^{\\langle t \\rangle}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables in model()\n",
    "## number of units for the pre-attention Bi-LSTM's hidden state \"a\"\n",
    "n_a = 32\n",
    "\n",
    "## number of units for the post-attention LSTM's hidden state \"s\"\n",
    "n_s = 64\n",
    "\n",
    "## post-attention LSTM cell\n",
    "post_attention_LSTM_cell = tf.keras.layers.LSTM(units=n_s, return_state=True)\n",
    "\n",
    "## output layer\n",
    "output_layer = tf.keras.layers.Dense(units=len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \n",
    "    # Initialization\n",
    "    outputs = []\n",
    "    \n",
    "    # Input layers\n",
    "    X = tf.keras.layers.Input(shape=(Tx, human_vocab_size))\n",
    "    \n",
    "    # Define initial hidden state and initial cell state for post-attention LSTM\n",
    "    s0 = tf.keras.layers.Input(shape=(n_s,), name='s0')\n",
    "    c0 = tf.keras.layers.Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Define pre-attention Bi-LSTM\n",
    "    a = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "        \n",
    "        ## perform one-step attention to get context vector\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        ## feed context to post-attention LSTM cell\n",
    "        s, _, c = post_attention_LSTM_cell(inputs=context, initial_state=[s, c])\n",
    "        \n",
    "        ## compute output\n",
    "        output = output_layer(inputs=s)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # Create Model instance\n",
    "    model = tf.keras.Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 37)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 30, 64)       17920       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30, 128)      0           bidirectional[0][0]              \n",
      "                                                                 repeat_vector[0][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[1][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[2][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[3][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[4][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[5][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[6][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[7][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[8][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30, 10)       1290        concatenate[0][0]                \n",
      "                                                                 concatenate[1][0]                \n",
      "                                                                 concatenate[2][0]                \n",
      "                                                                 concatenate[3][0]                \n",
      "                                                                 concatenate[4][0]                \n",
      "                                                                 concatenate[5][0]                \n",
      "                                                                 concatenate[6][0]                \n",
      "                                                                 concatenate[7][0]                \n",
      "                                                                 concatenate[8][0]                \n",
      "                                                                 concatenate[9][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 1)        11          dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "                                                                 dense[2][0]                      \n",
      "                                                                 dense[3][0]                      \n",
      "                                                                 dense[4][0]                      \n",
      "                                                                 dense[5][0]                      \n",
      "                                                                 dense[6][0]                      \n",
      "                                                                 dense[7][0]                      \n",
      "                                                                 dense[8][0]                      \n",
      "                                                                 dense[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  33024       dot[0][0]                        \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[1][0]                        \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "                                                                 dot[2][0]                        \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[1][2]                       \n",
      "                                                                 dot[3][0]                        \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[2][2]                       \n",
      "                                                                 dot[4][0]                        \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[3][2]                       \n",
      "                                                                 dot[5][0]                        \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[4][2]                       \n",
      "                                                                 dot[6][0]                        \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[5][2]                       \n",
      "                                                                 dot[7][0]                        \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[6][2]                       \n",
      "                                                                 dot[8][0]                        \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[7][2]                       \n",
      "                                                                 dot[9][0]                        \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[8][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           715         lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nmt_model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "nmt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "nmt_model.compile(optimizer=optmizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the inputs\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "\n",
    "# Swap the axis 0 and 1 in Y: in model(), need the outputs to be a list of Ty elemnts of shape (m, machine_vocab_size)\n",
    "y_1h_ = list(y_1h.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 17.2047 - dense_2_loss: 1.2642 - dense_2_1_loss: 1.0316 - dense_2_2_loss: 1.7153 - dense_2_3_loss: 2.7467 - dense_2_4_loss: 0.9027 - dense_2_5_loss: 1.3391 - dense_2_6_loss: 2.7086 - dense_2_7_loss: 1.1315 - dense_2_8_loss: 1.7354 - dense_2_9_loss: 2.6296 - dense_2_accuracy: 0.4696 - dense_2_1_accuracy: 0.7044 - dense_2_2_accuracy: 0.3246 - dense_2_3_accuracy: 0.0848 - dense_2_4_accuracy: 0.8561 - dense_2_5_accuracy: 0.3067 - dense_2_6_accuracy: 0.0647 - dense_2_7_accuracy: 0.7932 - dense_2_8_accuracy: 0.2756 - dense_2_9_accuracy: 0.1147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd8c540cd68>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_model.fit([X_1h, s0, c0], y_1h_, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_model.load_weights('./nmt_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-33 \n",
      "\n",
      "source: 5 April 09\n",
      "output: 2009-04-05 \n",
      "\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-20 \n",
      "\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10 \n",
      "\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09 \n",
      "\n",
      "source: March 3 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = np.array([str_to_int(example, Tx, human_vocab)])\n",
    "    source = np.array(list(map(lambda x: tf.keras.utils.to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "    prediction = nmt_model.predict(x=[source, np.zeros((1, n_s)), np.zeros((1, n_s))])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAGlCAYAAAAxlmW+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd7xdZZXw8d+66QFEqSpdpIiokISODRs6FrCj4qjYy1jGGdu8ysw7dmfesc04tsGKvTKgMqj0UA0QmqKAgo5KEUhIQpK73j/2vuTk5ux9zr035+bJvb/v53OTc86zn73X2aess9uzIjORJEllGdrUAUiSpA2ZoCVJKpAJWpKkApmgJUkqkAlakqQCmaAlSSrQwBJ0RHw+Iv4UEUsHtQxJkqaqQW5BnwQcPcD5S5I0ZQ0sQWfmWcBtg5q/JElT2cxNHUBEvBJ4JcDcefMW7rrLLl2nGx4eZmhofL8nplPfzS1e+5a9zH76to1FmMPDREvfFSvvaWybNXOI1WuGG9vnz53d2DaR5ytNpltvvZVbbrklurVt8gSdmZ8GPg2wcOGiPPeCi7tOd/45P+ewIx8zrmVMp76bW7z2LXuZ/fS95a5VjW1XX3oeD1lweGP7Xkf9bWPbe195CO/69AWN7f973kcb2y5efDaLDn1kY/ucWTMa26TJdMQhixrb/IkpSVKBTNCSJBVokJdZnQycD+wTETdFxAmDWpYkSVPNwI5BZ+Zxg5q3JElTnbu4JUkq0CY/i1vS5m3Luc1fI0ND0doeey5obpszv7X9qpvvamxbec/a1vYDd79vY5tUCregJUkqkAlakqQCmaAlSSqQCVqSpAINNEFHxBsjYmlEXBkRbxrksiRJmkoGOVDJ/sArgIOBRwBPjYi9BrU8SZKmkkFuQT8EWJyZd2fmGuBM4NgBLk+SpCkjMtuKxU1gxhEPAb4PHAasAM4ALs7MN4ya7t5ykzvuuOPCk7/2ta7zW7ZsGVtuueW4YplOfTe3eO1b9jL76Tvc8hVy97JlzG/pe/n1tza27bT1DG6+Y21j+947NV/LvGbV3cycM7+xff4cq1mpDG/927dyySUXT265ycy8OiI+CJwOLAMuA9Z0mW69cpNNZe1KLbVXWt/NLV77lr3MfvquXN2cRC9ZfDYLW8o+Pu2TX2hse+9TtuZdp97R2H7GBx7X2Hbbry9hmz0XNrY7UIk2BwM9SSwzP5eZCzLzUcBtwK8GuTxJkqaKgQ71GRE7ZOafImJX4JlUu7slSVIPgx6L+9sRsS2wGnhdZt4+4OVJkjQlDDRBZ2bzwSdJktTIkcQkSSqQ5SYlTciq1cONbZnZ3v7bK5vb7jmwtX3XbV/Y2LbsxiF23XZeY7u0OXALWpKkApmgJUkqkAlakqQC9ZWgI2K3iHh8fXteRGw12LAkSZreeiboiHgF8C3gP+uHdga+18/MLTcpSdL49LMF/TrgCOBOgMz8FbBDr06Wm5Qkafz6SdCrMvOekTsRMRPopwSW5SYlSRqnnuUmI+JDwF+AFwNvAF4LXJWZ7+rRz3KTliS070bsW2q8a1vqTa5Yvox5W7SUm/zlzY1tO207n5tvvbux/WF77dTYtvLuZcyd37zcmTO6VveTJl1bucl+EvQQcALwRCCAHwOfzT4KSUfECVS7yJcBVwErMvPNTdMvXLgoz73g4q5tpZbaK63v5havfcteZj9977h7dWPbFRefw8MWHdnYvvvj397Y9t6XHci7Pv+Lxvbrfvy+xrarLj2P/RYc3ti+7VZzGtukyXTEIYsmVA96HvD5zPwMQETMqB9r/mlby8zPAZ+r+70PuKnfoCVJms76OQZ9BlVCHjEP+J9+Zh4RO9T/j5SbPHmsAUqSNB31swU9NzOXjdzJzGURMb/P+VtuUpKkcegnQS+PiAWZeSlARCykOumrJ8tNSpI0Pv0k6DcB34yI39f3HwA8b3AhSZKkngk6My+KiH2BfajO4r4mM5tP25SmiNYLFbK9vbVrwnDLpUkT6RdNVw/1iLftUqlMWLO2uWTknFnNp7JERGv7Faf8c2Pbry9fzBWnPKux/cHP/Whj23ufuxtP/0Bz++2n/X1jm1SKfutBHwTsXk9/YESQmV8cWFSSJE1zPRN0RHwJ2BNYAqytH07ABC1J0oD0swW9CNivn4FJJEnSxtHPddBLgfsPOhBJkrROP1vQ2wFXRcSFwKqRBzPz6W2dImIucBYwp17OtzLzPROIVZKkaaOfBH3iOOe9CjiqHthkFnBORJyWmYvHOT9JkqaNfi6zOjMidgP2ysz/qUcRm9FHv6QqkgEwq/7zOLYkSX3op5rVK6jKQW6TmXtGxF7ApzLzcT1nXhXWuAR4MPDJzHxbl2ksN7kR+25u8Rbdt+Wj0atv26dq+bJlbDGOmPvp13QZ9CDjbet797JlzG/pu2Ztc+9VK5YxZ15z3yuv/1Nj2073m83Nt9/T2H7gXjs2tkmTqa3cZD+7uF8HHAxcAJCZvxopgtFLZq4FDoiI+wLfjYj9M3PpqGk+DXwaqnKTTWXtSi21V1rfzS3ekvu2/XhdfM6ZHHrko1v6Ni938blncugRzX0n0q9poJJe8bYNVHLheWdx8OGPamxf09L3ksVns/DQ5hF/b7mrOYn++vLF7PnwQxvbj/ngxxvb3vvc3XjXN25sbL/9NAdDVPn6OYt7VWbe+ymKiJmMcVd1Zv4F+Dlw9JiikyRpmuonQZ8ZEe8E5kXEE4BvAj/s1Skitq+3nImIecDjgWsmEqwkSdNFPwn67cCfgSuAVwGnAv/QR78HAD+LiMuBi4DTM/OU8QYqSdJ00s9Z3MPAZ+q/vmXm5cCB44xLkqRprZ+xuK+nyzHnzHzQQCKSJEl9j8U9Yi7wHGCbQQSzZji54+7ulSzXtrRBj1J6a5O/LG8+W7RNr75tZ8utWZvc3tK37czZNWuTW+5a1dg+3n7zZjdfwj48DMtXrWnuO6u573hLKPbTt22ume3rse35rB1O7lzR/J5a3XIJ0Jrh5Lbl43w/Die3tbwvmi6VWjvc/n6qOnfv3SvemUNNS4XhTJavWtvYPrelnCQ0Px+AnbeZ19j2u5nR2t5WMvL8c37umdra7PU8Bp2Zt3b83ZyZ/wYcNQmxSZI0bfWzi3tBx90hqi3qrQYWkSRJ6msX97903F4D3AA8dyDRSJIkoL+zuB87GYFIkqR1+tnF/Za29sz8140XjiRJgv7P4j4I+EF9/2lUdZ5/N6igJEma7vpJ0NsBCzLzLoCIOBH4Zma+fJCBSZI0nfVTbvIa4BGZuaq+Pwe4LDP33SgBjCo3+eWvntx1uhXLlzFvi/GVBlxx9zLmzR9fScJefdvW3sq7lzF3nMsdb99e/Yaayh0Bdy9fxvyWddzSddwlFAfdd7jl/d3rPdX20SjxtZ1I37bXttd6irb3VI9yky2XX2+yMqTSZJpouckvARdGxHep8tGxwBc3VnCd5SYfceDCfNiiI7tOd8XF59DUVs+nsW3pxeey/6IjxhVfr75tCbpX37YBNq6+9DwesuDwfkIcU7+2gUqWXHAOBxzSvI7bBioZbwnFfvq2reMLzj2TQ1r6tg1UcvlF5/Dwg5qfb9tAJVddci77LWx5X7S8H3u9Rk0566pLz2O/Xu+JhmTZK962gUp6ffbaBiq5ePHZLGopNzmn5T21qcqQSqXoZ6CS9wIvBW4H/gK8NDPf1+8CIuJ1EbGk/nvg+EOVJGn66GcLGmA+cGdm/lddRnKPzLy+n46Z+Ungk+OOUJKkaajnFnREvAd4G/CO+qFZwJcHGZQkSdNdP/WgjwWeDiwHyMzf41CfkiQNVD8J+p6sznhJgIjYYrAhSZKkfo5BfyMi/hO4b0S8AngZ8JlBBDNjKJjfcJbxUDS3QVVOr0lEtJ4t2qZX37YyiUMRzJ7R/BvotpXNpQOHM1lxT/cSf21nf68dTpatbD5zue3M5Kr8YnPfOTPbf8+1XdLU42q+1ufUOl9gzdrhxva2MolVec3m9l7Pt62MYtulR9B+uduMhjOqI2Bmy/sJ2tdV2yVNW8xpfo8PRbS2t8U0FO1naktq1s9Y3B+JiCcAdwJ7A+/OzNMHHpkkSdNYX2dxZ+bpEXEp8CjgtsGGJEmSGvdNRcQpEbF/ffsBwFKq3dtfiog3TVJ8kiRNS20HtPbIzKX17ZcCp2fm04BDqBK1JEkakLYEvbrj9uOAUwHqohnNZ+XUIuLzEfGniFjaa1pJkrS+tgT9u4h4Q0QcCywAfgQQEfOoBivp5STg6AlHKEnSNNSWoE8AHgq8BHheZv6lfvxQ4L96zTgzz8ITyiRJGpee5SYnNPOI3YFTMnP/lmnWKzf51ZO/1nW6XmUF255GrzKKbQbZd81w85GCVSuWM2de9zFh2p7rPSuXM3tu81gybdffrlyxjLnzmuOdOaO57yBLRra9Q3uVM1zbct13r+fbdinzpij92LPkKs3vjV7LbLr2Gnq/Pm3raVOVjLTcpDYXEy03OVCd5SYXLFyUBx32qK7TXXT+WTS1QftAJZcuPpsFLSXv2vTq2zZQyZILz+GAg5vL9N22vHmgkt9csZgHPezQrm1tg3rcsPQCdt//kMb2tkEjfrXkfPY64LDG9u23mt3YduF5Z3Hw4c2vT9uPil6vbdvgG73KGd62fHVjW6/n2zZQyZWXnMtDW8o3tunVtylZ9ir7CM3rqlfp063mNn8V9Hpt2wYq2VQlIy03qamgn6E+JUnSJOunmtUGP7u7PSZJkjaefragP97nY+uJiJOB84F9IuKmiDhhrMFJkjRdNR54iojDgMOB7SPiLR1N9wF6jn6fmcdNPDxJkqantpPEZgNb1tN01n++E3j2IIOSJGm6a0zQmXkmcGZEnJSZN05GMJnNZyi3tU3UrJbLhyLa22fMai+1N7+lTN+82fMa2343Y4id7te9vW09/H5mcz9ov2Tp+hnBtls2n6ndq4RiW3uPrgy1XOYTLUEH7ZcItZ2dPDQUre23t5xlP5zJ8lXNpTlXrW6+hG7tcHLbsuZ5N62LNWuTW1v6QfN7dTjby5De1VJmdPXaYX5/+8rG9q3nN49btHY4uePu5jPp7zOvZRshoe0y0F7vR2lz189lVidFbPgVmZlHDSAeSZJEfwn6rR235wLPApp/bkuSpAnrmaAz85JRD50bEWcOKB5JkkR/10Fv0/G3XUQ8Cbh/PzOPiKMj4tqIuC4i3j7haCVJmib62cV9CdW5RUG1a/t6qkIarSJiBvBJ4AnATcBFEfGDzLxq/OFKkjQ99LOLe49xzvtg4LrM/A1ARHwNeAZggpYkqYeeCToi5gKvBY6k2pI+B/iPzGy+7qKyE/C7jvs3Ac1VHCRJ0r16lpuMiG8AdwFfrh86DrhfZj6nR7/nAE/KzJfX948HDs7MN4yabr1yk1/5avdyk4Ms+9h2OWXPUnsty+1V8q5tzY+3fOMgyz5ujn3bLp3vVb5x7TjLgcLESoJOpF/Te7lXvG169W27Dr3XOp7R8uHrWTJyQGUupck00XKT+2TmIzru/ywiLuuj303ALh33dwZ+P3qiznKTBy5YlAsbSgdesvhsmtqgPdn1KhnZNhBJr1J7bV9Oi885k0OPfHRje9uX+OJzz+TQI7r3bRuopFfpxrb1dPH5Z7GopW/bc73g3DM5pCHeXnr1bfsR2ev1WdkyYEivcqBtA5W0lQOF9oFKbrr6QnZ+yMGN7U0Dlfz2ygvY9aHtO6Ga3su94m17L16/dDF77N/ct22gkl4lMtsGKun1+WkbqMRyk5oK+imW8YuIuPfTGRGHAOf20e8iYK+I2CMiZgPPB34wvjAlSZpe+tmCPgR4cUT8tr6/K3B1RFwBZGY+vFunzFwTEa8HfkxVXOPzmXnlxghakqSprp8EffR4Z56ZpwKnjre/JEnTVT8J+p8z8/jOByLiS6MfkyRJG08/x6Af2nknImYCCwcTjiRJgpYt6Ih4B/BOYF5E3Mm6ixruoT7remMbCpg7u3t5xqGh5rae8x2CeePsGwEzZ/TzO6Zb5/GXYIxoPpu3tTRjwKyZ44s3Ama39P3DX5ovfV+zNvnTnasa2z985m8a2w6dsZJv/fDqxvaZLWfZH8RK3nnatY3t73/Kvo1tQ9H+vthy7vzGtptmDrHLts3tbf583RAPvv/YLwH64y+HeNAO47tUaiLx3jxziF23G1/fGUPRepZ3qx6fH2mqa/w2zsz3Z+ZWwIcz8z6ZuVX9t21mvmMSY5Qkadrp5xj0aRGxwYWmmXnWAOKRJEn0l6D/ruP2XKoxti8BjhpIRJIkqa9iGU/rvB8RuwAfGlhEkiSpr7O4R7sJ2H9jByJJktbpp5rVx1k3hPMQcADQz1jckiRpnPqpZvXXHXfXADdkZj9jcfcXwKhqVid/rXs1q4lUp5lOfQe5zNVrmt8rK1csY+685r7/u6z5EqwtuIflzG5sb7vQplffnbae29jWs1LZgKollfja2lfaNCZazerrwIOptqJ/3Ucd6DHprGa1cOGibKpAM5HqNNOp7yCX2XYd9K+WnM9eBxzW2P691uugf8vitbs2trdfB30jF7FbY/szj2i+DrpXFa226l1T7bW1r1SexmPQETEzIj5Edcz5C1T1oH8XER+KiL5HHoiI10XEkvrvgRMPWZKkqa/tJLEPA9sAe2Tmwsw8ENgTuC/wkX4XkJmfzMwD6r8N6kFLkqQNtSXopwKvyMy7Rh7IzDuB1wBPGXRgkiRNZ20JOrPLGWSZuZZ1Z3VLkqQBaEvQV0XEi0c/GBEvAq4ZXEiSJKntLO7XAd+JiJdRDe2ZwEHAPODYSYhNkqRpqzFBZ+bNwCERcRRVTegATsvMMyYrOJVluy2brze+fka0tm+zRfNvwZn3BNvMbW4/++o/N7Y9dKe1XHbzbY3tLVdKET3a1Z/WsRSyvd1yklKzfsbi/inw00mIRZIk1cYzFrckSRowE7QkSQUaaIKOiKMj4tqIuC4i3j7IZUmSNJUMLEFHxAzgk8CTgf2A4yJiv0EtT5KkqWSQW9AHA9dl5m8y8x7ga8AzBrg8SZKmjJ7lJsc944hnA0dn5svr+8cDh2Tm60dNZ7nJjdh3kMtse6v0Kt34v3c1l5ucm6tYGXMa2+9auaaxbdtZa7h1dfPFCHtvv0VjW891ZbnJ/vq2vC82t3UsTbaJlpscr24L7DZ0qOUmN2LfQS5z9ZrhxraLzj+Lgw57VGP7B39+XWPbfvdcz1Wz92hsP/vXzddBv2Cnv/DVm+/b2P6jYw9vbFt8zpkcemRzucm2a3Sn2ms7kb5tP/I3t3UslWSQu7hvAnbpuL8zYDUrSZL6MMgEfRGwV0TsERGzgecDPxjg8iRJmjIGtos7M9dExOuBHwMzgM9n5pWDWp4kSVPJII9Bk5mnAqcOchmSJE1FjiQmSVKBTNCSJBVooLu4NbXMmtn8ey6ivf2dR+3V2Lb43N9zzBHN7due+NnGtme85BFc+PUzG9tXv+awxrYEVq9tvkRo9szxl0KcSAnGpqZMGB5uH7egqTUT1rb0bXumvZY7kYqRE1lP7TMe3zrWxrOpKolOpRKmbkFLklQgE7QkSQUyQUuSVKBBl5t8Y0QsjYgrI+JNg1yWJElTySDLTe4PvIKqqtUjgKdGRPOZQJIk6V6D3IJ+CLA4M+/OzDXAmcCxA1yeJElTxiDLTT4E+D5wGLACOAO4ODPfMGq6e8tNAvsA1zbMcjvglnGGM536bm7x2rfsZU7HvtJk2i0zt+/WMLAEDRARJwCvA5YBVwErMvPN45zXxZm5yL7lLdO+k9N3c4t3c+0rlWKgJ4ll5ucyc0FmPgq4DfjVIJcnSdJUMdCRxCJih8z8U0TsCjyTane3JEnqYdBDfX47IrYFVgOvy8zbJzCvT9u32GXad3L6bm7xbq59pSIM9Bi0JEkaH0cSkySpQMUn6Pr4tSRJ00rRCToingKcERE7bepYJkNE7BhTqVbaFOVrJGkyFJugI+JJwEeA4zPz5oiY1Fgn+iUcEVuPcfqdgH8AjtsUCSAidouIuZO4vH0i4rCImBURM8bQb6+IWBQRM8bSb2OIiJ3rkx53nszlbirjXb/jfY0m8tpGxEMj4tH16yNNCYM+i3tcIuKJwBeBs6munyYzhyMicoxntUXEkcB+wGfG2PeBwM0RMbMeqnQsy3wtsFVE/Edm3tlnt98DlwAHAqsi4jvjeK7zMnPFWPrU/XYA3gq8v45joCLimcD7gJvrv4sj4qRe6yoijgH+EbgOuAm4NiK+kJnLJyHmZwBvB/4IPCAiTgPel5n3jGEeD8nMq/ucdjbw4My8KiIeB1yVmX8YT+xjFRF7Z+YvM3NtRMzIzLVj6Duu12gir21EPBn4IPAbYFZEnJCZ/9tvzFKpituCrr+MPgG8BTgPeFmdZMnM7HfrsmOL+0HAw4EXjaHv64FPRcQHgNdGxJwxxP8q4K+Br2bmnRHR80dQxw+PYWBf4G3AM8ayJV3H/KGIeP9Yt96phkTcDfibMfYbs4iYBTwPOCEzH0c1HOwuwN9HxH1a+m0LvAo4LjOfBVwGvBR4c0RsNeCYHwt8GHg98BLgeOBo4D397tmJiNcAH46IHftc7K7Av0XEl6g+C33/EJiIiHgqsCQivgowkqT77Duu12gir21EPAb4KPDyzDyGaj3t30+8UumKS9DAncBLMvMrwH9TXUP9VxFxBIwpSe9Z//9lqi3xA4EX9+pb/5J/LtWX8CHA3pm5qp/AI2Ie8GTg3cDd9ZfyJ+v/G9XP6YXAG4B3Uf0weSzwrH6ea73F/hzgA8DLgI9HH5XDIuKB9dbSMFXy2TEi9u3VbyO4DzAS33eBU4DZwAtanu8aYEvg/gCZ+XngRmB74KkDjRYOBz6WmZcAKzPzl1Q/Mp4MvLNX54h4OvBqqrEA/tjPAjPzOuBy4BnAaZl5a73rd2CHPyJiC6r3wZuAeyLiy3Us/Sbp8b5GE3lt/wi8KjMvjIj7U31mXx8R/xkRz/Z8AW3OikvQmXlRZp4XEUOZeS3Vru7VVOUqD6+nad31G9WZ36dHxPF18vk28AvghcBLe3xotwb+DTimXu5b6nnu3UfsK4BTqXYVf55qq/RKYP96l2WbfYBvZOblwN9R7ep7A/Cctnjrrc4FwPOBZ1E9T4CPtSXp+sv476j2FLwS2ApYBexUtw/kiy0zVwP/CjwzIh5Zvz7nAEuAI1v63QF8her1Oz4i3guspBrj/QmDiLVjHexMVXwBqsMPMzLzRqqt6cdHxA491tcDga9n5o31HoR+fQp4LdVepBdm5tr6x9yWY3wqfal3J78M+CrVIY+5nUm6j/7jeo0m8tpm5tWZ+bP67gnAv9db0oupfrRu19hZKl1mFv9HtbX1HuBjwCF99nkacCnVbrORx04F/gXYuqXfo4FfA2d3PPY3wIeAWX0sdy5wELBNff844GfA/B79jgG+Bzy047FzgPcCW/XoO4eq5vbP6vtBtdv6n4DZPWJdAHydasv9j8BFwE4Dfj3nUm2pfRp4VMfjPwUOaOm3NdWPrP8C/l/H46cA9xlgvI8D/gdYWN8fAmZRJd5vA1v06P9k4DRgn47HjgeOGcN7+TLgr4An1q/rzEG+RvVyt62f35fr+wuAfXv0GddrNIjXtv68Lxj0evLPv0H9FXmS2GiZ+auI+DpVPenf9NnnhxGxFvhAvev5NqpjvB/J6hd7k0uojosO18e3dqU6pvzXWW399VruSuCiiBiKqprXm6h+JNzdo+vPqRL7cRHxU2Ak5o9n5l09lrkqIu4GZkbEw6iO6f4I+Gy2nMRUx3ppvQU9hyrxHFA/55s7jo1vVJm5MiK+AiTwjnq3+ipgR6DxRKj6dftKRJyc1ZY3EfFiYBug7xOZxmEx1Y+l50UEWe3qHq7PjdiGKlm3ORc4AvjriDiPam/F31D9eOupfi+vpvqReA/w4hzjiYvjkdVu9VdRHTu/BphBdeilrc+4XqOJvraj36sR8Syq99PAT3qUBmWzGuozImb1kyRH9Xk01dmhdwNvz2oXcq8+DwCeXv/dCnw4M68Y43LnUx2nXJz9n7n7QKqiIs+kOi73t/0uN6oT2d4EPJ7qi+m5mXnNWGKu5/Muqvqkr+w58QTVu/2PoDpBaCXw0cz8RXuv9fq/jGpX7PPG+vqMVVSXwb0cOAo4nypRPpvqx9dlffR/ANXx5KcDdwDv7+e9OGoe2wNk5p/HFv3ERMSbqU5cfMI4Pgfjeo0m0G8O8CKqQ1PPy8ylY4lXKslmlaDHq06WmWO8BGnkeOFYfxR09B/XFmh9fDgyc9kY+82iOtFmODNvHmPfyMyMiOdTnUF7zFjX13jVJyDlyJbTGPrtRnXY4brBRLbB8uYBi4AnUR1COC2r8yTGMo/ZAG17NkoSEfcDvkH1Y3FMPyjq/uN6jSbQbxbVcetfj/W1kUozLRK0+lOf6PRU4Hq3PDQiIubWh0MkTSITtCRJBSruMitJkmSCliSpSCZoSZIKZIKWJKlAJmhJkgpkgpYmUUSM6dr2Pue5e0S8oKFtKCI+FhFLI+KKiLgoIvbY2DFI2vg2i6E+JbXaHXgBVZGL0Z5HNWb4w7Oqqb4zMPD62ZImzi1oaROIiMdExM8j4lsRcU1EfGWkIlZE3BARH4yIC+u/B9ePnxQRz+6Yx8jW+AeAR0bEknpYzk4PAP4wMkpbZt6UmbfX/Z8YEedHxKUR8c2RKlkRcXQd0zn11vcp9eMnRsRbO5a/NCJ2r2+/qI51SVSlHmeMxBgR742IyyJicdT1sCNix4j4bv34ZVFXqmuajzQdmaClTedAqvHT9wMeRDUu+Yg7M/Ng4BNU5U/bvJ2q+toBmfn/RrV9A3hanfD+JSIOBIiI7YB/AB6fmQuAi4G3RMRc4DNUFbQeSV2juU1EPIRqS/2IzDyAqrjFC+vmLajGo38EcBbwivrxjwFn1o8vAK7sMR9p2nEXt7TpXJiZNwFExBKqXdXn1G0nd/w/Oun2LTNvioh9qIp8HAWcERHPoaqWth9wbr3hPpuqCMi+VEO9/qqO68tAr8IpjwMWUlVxo573n+q2e6hKRkJVKW6kvvNRwIvrGNcCd0TE8S3zkU+2y5QAABQESURBVKYdE7S06azquL2W9T+P2eX2Guq9XvXu8Nn9LCQzV1HVoz4tIv5IVXv8J8DpmbleycuIOGDUsjvdu/za3JFuwBcy8x1d+qzuKBgz+jmO1jYfadpxF7dUpud1/H9+ffsGqi1MqEpXjtShvouqxvQGImJBXcaUiBgCHg7cSFXj+oiO49vzI2Jv4Bpgj4jYs55FZwK/gWp3NBGxABg5G/wM4NkRsUPdtk1djarNGcBr6ulnRMR9xjkfacoyQUtlmhMRFwBvBEZO/PoM8OiIuBA4hHVnY18OrKlPthp9ktgOwA8jYunIdMAn6prSLwFOjojLqRL2vnXVqlcC/x0R51Al8xHfBrapd8e/BvglQGZeRXU8+yf1vE6nOjmtzRuBx0bEFVS7vh86zvlIU5bVrKTCRMQNwKLMvKWAWB4DvDUzn7qpY5GmG7egJUkqkFvQkiQVyC1oSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQCZoSZIKZIKWJKlAJmhJkgpkgpYkqUAmaEmSCmSCliSpQDM3dQCbqyc+6ei85ZZbek6X9/7T0NbUCGRz04Y9W5fRMFG2di1oWdnYb4PHszmObvPo9vo09Rgd1+j5dW9vmFsf/btHAZmta3qD9033ddR9jfbu271na7/s8Ro0vp+6rKTOeXR5Yj0/b91WRkPbWKdfb6q2D++9n4X2lb1e+xjXUecHrttr2DZ94wI36NftQz065i592r5MOpafK/7848w8ukuw04YJepxuveUWzl188XofkKR6D+eoD0d2fCA73+Od02au/34embbz89LZf9181+/fuazOz0KvuLpOO4bntTGXNdyRBEbahzdYL9UDw6PXYcLweutk3TobHrVOM5Nh1n2ZZsdjI+2d068f10jfjras/r83rlGxDHe0j9zPjumHRz+vjnmPvl/Ne/SyO2Ibfb/zeea6Pp3Ps/M55nrPY/1pO+NOus+r83mO9Ol8/brOqyGuHDWvDe+3T9/ftBv2HR7uPxY2mNeGbZ3tG2P68cyrCny44wM5vO6xrve73G7qOzzS3uf0Te317ZVLPrkd05y7uCVJKpAJWpKkApmgJUkqkAlakqQCmaAlSSqQCVqSpAKZoCVJKpAJWpKkApmgJUkqkAlakqQCmaAlSSqQCVqSpAKZoCVJKpAJWpKkApmgJUkqkAlakqQCmaAlSSpQZOamjmGzFBE/Arbb1HG02A64ZVMH0cL4xq/k2MD4Jsr4Krdk5tGTsJximaCnqIi4ODMXbeo4mhjf+JUcGxjfRBmfRriLW5KkApmgJUkqkAl66vr0pg6gB+Mbv5JjA+ObKOMT4DFoSZKK5Ba0JEkFMkFLklQgE/RmLCKOjohrI+K6iHh7l/Z9I+L8iFgVEW8tML4XRsTl9d95EfGIwuJ7Rh3bkoi4OCKOLCm+jukOioi1EfHskuKLiMdExB31+lsSEe8uKb6OGJdExJURcWZJ8UXE33Wsu6X1a7xNQfFtHRE/jIjL6vX30smKbdrITP82wz9gBvBr4EHAbOAyYL9R0+wAHAS8F3hrgfEdDtyvvv1k4ILC4tuSdedpPBy4pqT4Oqb7KXAq8OyS4gMeA5wyme+7McZ3X+AqYNf6/g4lxTdq+qcBPy0pPuCdwAfr29sDtwGzN8XrPVX/3ILefB0MXJeZv8nMe4CvAc/onCAz/5SZFwGrC43vvMy8vb67GNi5sPiWZf3tA2wBTOYZlT3jq70B+Dbwp0mMDfqPb1PpJ74XAN/JzN9C9XkpLL5OxwEnT0pklX7iS2CriAiqH7O3AWsmMcYpzwS9+doJ+F3H/Zvqx0ox1vhOAE4baETr6yu+iDg2Iq4B/ht42STFBn3EFxE7AccCn5rEuEb0+/oeVu8CPS0iHjo5oQH9xbc3cL+I+HlEXBIRL5606Mbw+YiI+cDRVD/EJks/8X0CeAjwe+AK4I2ZOTw54U0PMzd1ABq36PJYSdfM9R1fRDyWKkFP5jHevuLLzO8C342IRwH/F3j8oAOr9RPfvwFvy8y11UbMpOonvkuB3TJzWUQ8BfgesNfAI6v0E99MYCHwOGAecH5ELM7MXw46OMb2+X0acG5m3jbAeEbrJ74nAUuAo4A9gdMj4uzMvHPQwU0XbkFvvm4Cdum4vzPVL9lS9BVfRDwc+CzwjMy8dZJigzGuv8w8C9gzIiarQEo/8S0CvhYRNwDPBv49Io6ZnPB6x5eZd2bmsvr2qcCswtbfTcCPMnN5Zt4CnAVM1omKY3n/PZ/J3b0N/cX3UqpDBJmZ1wHXA/tOUnzTggl683URsFdE7BERs6k+xD/YxDF16hlfROwKfAc4fpK2WsYa34Pr42tExAKqk2Um60dEz/gyc4/M3D0zdwe+Bbw2M79XSnwRcf+O9Xcw1fdNMesP+D7wyIiYWe9GPgS4uqD4iIitgUfXsU6mfuL7LdXeByJiR2Af4DeTGuUU5y7uzVRmromI1wM/pjrj8vOZeWVEvLpu/1RE3B+4GLgPMBwRb6I6E3Pgu6D6iQ94N7At1ZYfwJqcpCo5fcb3LODFEbEaWAE8r+OksRLi22T6jO/ZwGsiYg3V+nt+SesvM6+Oqmzs5cAw8NnMXFpKfPWkxwI/yczlkxHXGOP7v8BJEXEF1S7xt9V7IrSRONSnJEkFche3JEkFMkFLklQgE7QkSQUyQete9aAcGRH7djy2e0S0njjTzzQbU0S8JCI+sZHmFRHx04i4T31/bcfYx9+sz+4dy/yWjXH6k6LLGNoRsSgiPlbfvvf5RsSrRwbUqB9/4FiWN1ZRjVV9+ATn8c5x9HlORFwdET8b9fjuEfGCjvsTei/U6/8x9WAlu4+j/771++UXEbEwIl473ljGsMwT6+d9UkQ8pn7saxExWdeYa5KYoNXpOOAcqksqpounAJd1nNm+IjMPyMz9gXuAV3dOXCf0gX9uMvPizPybLo9/KjO/WN99CTDQBE01nvaEEjTVmM1jdQLVZWOPHfX47lRDdJbiGOD7mXkg1SVkA0/QDf4D+PtNtGwNiAlaAETElsARVF+MXRN0/av9+xHxo6iq3Lyno3lGRHwmqqo2P4mIeXWfV0TERVEN9/jt0VukETEUETdExH07HrsuInaMiKdFxAX11sn/1Ndajo5pvS3Qzi3YqKoBXRRVRap/bHjqL6T5GtOzgQfXW21XR8S/U42OtUtEHBcRV9Rb2h8cFdO/RMSlEXFGRGzfx3p4fEScHRG/jIin1tM/JiJO6fJ8T4yIt9bPeRHwlXoL7q8i4rsd0z0hIr7Tpf/j6vV5RUR8PiLm1I/fEPUgIvXW+8gW5auBN9fLeGS9vj/VJd71tmQj4pT6OXwAmFf3/0qXeDZYj1FVvToS+FREfHhUlw9QXbu8JCLeXD/2wPo9+auI+FDHvJ8YVTW3S6PaG7Ll6OUDd1D9ELsNWBsRM+rnuLSO6831vA6IiMX1e+m7EXG/qEZHexPw8qi29D9ANZjNkoj4cP38z4yIb9Tr6gNRVXC7sJ73nvW8u77PI+Jj9bogIp4UEWdF9eNwGdVlayOxQ/VefXxEeOnsVLKpq3X4V8Yf8CLgc/Xt84AF9e3dgaX17ZcAf6C6dnkesJQqSexONUj+AfV03wBeVN/etmMZ/wy8ocuyPwq8tL59CPA/9e37se5SwJcD/9IRxyfq2yfRUcUJWFb//0Tg01TXZw4BpwCP6rLsG4GtuvSfSZW4X1M/v2Hg0LrtgVSDNGxfT/dT4Ji6LYEX1rff3RFn1/VQx/+jOsa9qEZwmktHJahRz/dE6spkwM+BRfXtAK4Btq/vfxV42qjnOpdqfOW96/tfBN5U374B2K6+vQj4+ejl9Yj33hjr6U4BHtO5Trus+7b1eO9zG9Xn3vXSsW5+A2xdx3Ej1QhY21GNDLZFPd3bgHf38TlYCJzecf++9f+XA4+ub/8T8G9dXo/dqT8rHbH+BXgAMAe4GfjHuu2NHfNoep/PB64EHgtcC+zZI/bTgYWb+rvEv4335xa0RhxHVbGG+v/jGqY7PTNvzcwVVKOAjYyffX1mLqlvX0L1ZQWwf721dQXV1mq3gglfB55X335+fR+q4QV/XPf9u4a+TZ5Y//2Caqt3X7qPA71NZt7VcX9eRCyhGuDlt8Dn6sdvzMzF9e2DqBLYnzNzDfAV4FF123BH/F9m3fppWw/fyMzhzPwVVbIZ83CJWX1Dfwl4Ub034jA2LD6yD9XrNDJq2xc64h6LCcdba1uPY3FGZt6RmSupykfuBhwK7AecW7+ef10/3stvgAdFxMcj4mjgzqhG87pvZo7Uix7LersoM/+Qmauoyjf+pH78CtZ9Rrq+zzPzbuAVVIn3E5n56x7L+hODP+ShSeTuEBER21INeL9/RCTVyEEZEd2OaY0e2Wbk/qqOx9ZSbWFDtcV1TGZeFhEvodqqGO18ql3J21Md0/vn+vGPA/+amT+I6mSYE7v0XUN9qCYigmo4Tqi2KN+fmf/Zpc96/SNiKNdV4VmRmQd0TlDNls6RnMZSmWJk/ZxE83poWqdj9V/AD4GVwDfrpNepLe571yPVlmibbvF29u9nHr3iGYvR772Z9bxPz8ymH5pdZebtEfEIqkIQrwOeC7y5vVffsQ133B9m3fdv2/v8YVTHtvtJvHOpdn1rinALWlANyfjFzNwtq7Gdd6Ea+L5bdaknRMQ2UR1jPgY4t8e8twL+EBGzqLYcN1Bv/X0X+Ffg6lxXNGNrqt2CUG0BdXMD1W5JqOrVzqpv/xh42chxx4jYKSJ26NL/Wqqi9GNxAfDoiNguImZQ7W0Y2boaolqfUJ3MdE59u209PCeqY/F71rFc22ccd9XzBSAzf09V0OAfqH4QjHYNsHtEPLi+f3xH3Dewbj0+q2kZLfHeABxQP74LVT3hEavr5z1a23ps0i2ebhYDR4w814iYHxF79+pUH4cfysxvA/+H6lDPHcDtEfHIerLO9Tae2Ebr+j6PiN2AvwUOBJ4cEYf0mM/eVLvENUWYoAXVF+N3Rz32bbqfLXsO1a7UJcC3M/PiHvP+P1RfxKdTJYgmX6c6Dv71jsdOBL4ZEWcDTWP8fobqS/5CquPXywEy8ydUx2HPr3cdfovuX57/Tfet+kaZ+QfgHcDPgMuASzNz5ESz5cBDI+ISqr0S/1Q/3rYerqX6wj8NeHW9q7YfJ1GdSLWk/sEE1W7i32XmVV3iXklVgeib9ToZZl0t6X8EPlqv67Ud3X4IHDtyklhLvOdS/ai7AvgI1WGFEZ8GLh99kliP9djkcqq9Hpd1nCS2gcz8M9Xx6ZMj4nKqhN3PrvidgJ/Xu8VPquODKnF+uJ7XAax7XTuXeSvVLvWlXU5ua3Mio97n9d6gz1Ed3/491cmbn42Irnsm6hPLVtTrVFOEY3Grb/Wu2UWZ+fpNHcvGEhEPoNp78IRNHcvGENWZ1L/IzM/1nHh88z+J6iStbw1i/hqf+sfKnYN63bVpuAWtaa3e4vhM1AOVbM7qrfaHU52cpunlL1Qnr2kKcQtakqQCuQUtSVKBTNCSJBXIBC1JUoFM0JIkFcgELUlSgf4/OWUi/0rsKigAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x612 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(nmt_model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
